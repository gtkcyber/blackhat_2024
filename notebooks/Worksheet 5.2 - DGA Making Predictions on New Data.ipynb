{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "076c9974-38cc-4dc3-889c-c1365d12b765",
   "metadata": {},
   "source": [
    "# Worksheet 5.2 - DGA Making Predictions on New Data\n",
    "\n",
    "Once you have trained a model, you will need to build a pipeline for any new data to be pre-processed and feature engineered so that i can then be put into the model to get a prediction. This Notebook is to show how we would build that pipeline and load the trained model to make these predictions for new data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798014d7-09d7-4bdf-908b-a25004fdaaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a20aa9-c54e-4ad3-8cbb-1a351dd22d9f",
   "metadata": {},
   "source": [
    "## Making a Prediction\n",
    "The code below demonstrates how you will go from an unknown raw domain to predicting whether it is DGA or not.  The key thing is that you have to regenerate all the features, and create a 1 row dataframe of all your features which is then passed to the model. \n",
    "\n",
    "### Define all the functions we used for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42881b52-be25-4623-b0be-cb8b10f1471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def H_entropy (x):\n",
    "    # Calculate Shannon Entropy\n",
    "    prob = [ float(x.count(c)) / len(x) for c in dict.fromkeys(list(x)) ] \n",
    "    H = - sum([ p * np.log2(p) for p in prob ]) \n",
    "    return H\n",
    "\n",
    "def firstDigitIndex(s):\n",
    "    for i, c in enumerate(s):\n",
    "        if c.isdigit():\n",
    "            return i + 1\n",
    "    return 0\n",
    "\n",
    "def vowel_consonant_ratio (x):\n",
    "    # Calculate vowel to consonant ratio\n",
    "    x = x.lower()\n",
    "    vowels_pattern = re.compile('([aeiou])')\n",
    "    consonants_pattern = re.compile('([b-df-hj-np-tv-z])')\n",
    "    vowels = re.findall(vowels_pattern, x)\n",
    "    consonants = re.findall(consonants_pattern, x)\n",
    "    try:\n",
    "        ratio = len(vowels) / len(consonants)\n",
    "    except: # catch zero devision exception \n",
    "        ratio = 0  \n",
    "    return ratio\n",
    "\n",
    "# ngrams: Implementation according to Schiavoni 2014: \"Phoenix: DGA-based Botnet Tracking and Intelligence\"\n",
    "# http://s2lab.isg.rhul.ac.uk/papers/files/dimva2014.pdf\n",
    "\n",
    "def ngrams(word, n):\n",
    "    '''\n",
    "    Extract n ngrams from each word and return a regular Python list\n",
    "    \n",
    "    Input \n",
    "    word: (string) or a (list) of strings\n",
    "    n: (integer) or a (list) of integers, lenght of ngram\n",
    "\n",
    "    Output: \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    list_ngrams = []\n",
    "    if isinstance(word, list):\n",
    "        for w in word:\n",
    "            if isinstance(n, list):\n",
    "                for curr_n in n:\n",
    "                    ngrams = [w[i:i+curr_n] for i in range(0,len(w)-curr_n+1)]\n",
    "                    list_ngrams.extend(ngrams)\n",
    "            else:\n",
    "                ngrams = [w[i:i+n] for i in range(0,len(w)-n+1)]\n",
    "                list_ngrams.extend(ngrams)\n",
    "    else:\n",
    "        if isinstance(n, list):\n",
    "            for curr_n in n:\n",
    "                ngrams = [word[i:i+curr_n] for i in range(0,len(word)-curr_n+1)]\n",
    "                list_ngrams.extend(ngrams)\n",
    "        else:\n",
    "            ngrams = [word[i:i+n] for i in range(0,len(word)-n+1)]\n",
    "            list_ngrams.extend(ngrams)\n",
    "\n",
    "    return list_ngrams\n",
    "\n",
    "def ngram_feature(word, common_dict, n):\n",
    "    '''\n",
    "    Takes (word) as input, splits it into (n) ngrams and then looks up and counts where\n",
    "    these ngrams are found in the (common_dict). Function returns the normalized sum of all \n",
    "    ngrams that were found in the (common_dict). \n",
    "    \n",
    "    For example, ngram_feature('facebook', ngram_common_dict, 2) will return 171.28\n",
    "    \n",
    "    Input \n",
    "    word: (str) or (list) of strings (domain in our case)\n",
    "    common_dict: (dictionary) that contains the count for most common english words\n",
    "    n: (int) or (list) of the ngram length example: 1,2,3\n",
    "    \n",
    "    Output\n",
    "    feature: (float) a normalized sum of ngram count found in the common_dict\n",
    "    '''\n",
    "\n",
    "    # get a list of matching ngrams from common dict. \n",
    "    list_ngrams = ngrams(word, n)\n",
    "    \n",
    "    count_sum=0\n",
    "    \n",
    "    for ngram in list_ngrams:\n",
    "        if common_dict[ngram]:\n",
    "            count_sum += common_dict[ngram]\n",
    "    try:\n",
    "        feature = count_sum/(len(word)-n+1)\n",
    "    except:\n",
    "        feature = 0\n",
    "    return feature\n",
    "    \n",
    "def average_ngram_feature(word):\n",
    "    \n",
    "    '''\n",
    "    Takes a word (word) as input, uses the ngram_feature and ngrms functions (above) to create ngrams, \n",
    "    get the sum of the ngram count found in the common_dict for 1, 2 and 3-grams. \n",
    "    Then, Calculate the average of these three results.\n",
    "    \n",
    "    Input: \n",
    "    Output: average of the list \n",
    "    '''\n",
    "    ngram_counts = []\n",
    "    num_of_grams = [1,2,3]\n",
    "    for n in num_of_grams:\n",
    "        ngram_counts.append(ngram_feature(word, ngram_common_dict, n))\n",
    "                  \n",
    "    return sum(ngram_counts)/len(ngram_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025774af-dbc8-4094-8b72-7e6854a114fa",
   "metadata": {},
   "source": [
    "## Load the dictionary of common ngrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e222a12b-e67d-4619-b6ad-5161e3b034bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/first20hours/google-10000-english\n",
    "with open('../data/d_common_en_words' + '.pickle', 'rb') as f:\n",
    "        ngram_common_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bb1452-61b7-4adc-8f8a-a2f76e4e3516",
   "metadata": {},
   "source": [
    "## Load the model that you trained in the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e36ee8-5d65-4345-bea9-c18bcf365f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the trained model\n",
    "with open('../data/dga_decision_tree.sav', 'rb') as file:\n",
    "    clf = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c7b4f9-e68e-41da-8bfb-0b886304b79d",
   "metadata": {},
   "source": [
    "## Define function to create the pipeline and get prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2e34a7-704f-47eb-bfc4-071fc4bd922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_dga(domain, clf, common_dict):\n",
    "    # Function that takes new domain string, trained model 'clf' as input and\n",
    "    # dictionary ngram_common_dict of most common english words\n",
    "    # returns prediction\n",
    "    \n",
    "    domain_features = pd.DataFrame()\n",
    "    # order of features is ['length', 'digits', 'entropy', 'vowel-cons', firstDigitIndex, 'ngrams']\n",
    "    domain_features.loc[0,'length'] = len(domain)\n",
    "    pattern = re.compile('([0-9])')\n",
    "    domain_features.loc[0,'digits'] = len(re.findall(pattern, domain))\n",
    "    domain_features.loc[0,'entropy'] = H_entropy(domain)\n",
    "    domain_features.loc[0,'vowel-cons'] = vowel_consonant_ratio(domain)\n",
    "    domain_features.loc[0,'firstDigitIndex'] = firstDigitIndex(domain)\n",
    "    domain_features.loc[0,'ngrams'] = average_ngram_feature(domain)\n",
    "    \n",
    "    pred = clf.predict(domain_features)\n",
    "    return pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a566486-f758-4de3-a977-71794e100aac",
   "metadata": {},
   "source": [
    "# Make a prediction on a domain \n",
    "Get a prediction using the ```is_dga``` function defined above for the following domain\n",
    "\n",
    "```\n",
    "adfajdskflajlkdsfjaksdjf;lakjsdfkajdsf8989dsf32.com\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe718c8f-6e30-4659-9146-8c6afc8650e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c156380-7299-4c30-98a9-f83d9bde9cec",
   "metadata": {},
   "source": [
    "A few more domains to try\n",
    "\n",
    "- spardeingeld\n",
    "- google\n",
    "- blackhat.com\n",
    "- 1vxznov16031kjxneqjk1rtofi6\n",
    "- lthmqglxwmrwex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a2617-0abc-4237-9ca0-7237cb2ae381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
