{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T00:57:06.144447Z",
     "start_time": "2023-08-02T00:56:57.002456Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model_selection\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#import gensim\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential, Model, model_from_json, load_model\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m regularizers\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Load Libraries - Make sure to run this cell!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, os\n",
    "from string import printable\n",
    "from sklearn import model_selection\n",
    "\n",
    "#import gensim\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model, model_from_json, load_model\n",
    "from keras import regularizers\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda, Flatten, Concatenate\n",
    "from keras.layers import Input, ELU, LSTM, Embedding, Convolution2D, MaxPooling2D, \\\n",
    "BatchNormalization, Convolution1D, MaxPooling1D\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "import np_utils\n",
    "from keras import initializers\n",
    "from keras import backend as K\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"../img/GTK_Logo_Social Icon.jpg\" width=175 align=\"right\" />\n",
    "\n",
    "## Featureless Deep Learning\n",
    "\n",
    "This notebook shows three commonly used neural network architectures to detect malicious URLs using **featureless Deep Learning**. [Keras](https://keras.io/) is used as high-level API for [tensorflow](https://www.tensorflow.org/) backend). \n",
    "\n",
    "Please refer to the slides (```Module 9 Featureless Deep Learning```) for additional info!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess raw URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T00:57:09.307337Z",
     "start_time": "2023-08-02T00:57:09.090397Z"
    }
   },
   "outputs": [],
   "source": [
    "## Load data URL\n",
    "\n",
    "DATA_HOME = '../data/'\n",
    "df = pd.read_csv(DATA_HOME + 'url_data_mega_deep_learning.csv')\n",
    "df.sample(n=25).head(25) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T00:57:12.776357Z",
     "start_time": "2023-08-02T00:57:09.964740Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initial Data Preparation URL\n",
    "\n",
    "# Step 1: Convert raw URL string in list of lists where characters that are contained in \"printable\" are stored encoded as integer \n",
    "\n",
    "# Step 2: Cut URL string at max_len or pad with zeros if shorter\n",
    "max_len=75\n",
    " \n",
    "# Step 3: Extract labels form df to numpy array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T00:57:14.220289Z",
     "start_time": "2023-08-02T00:57:14.165551Z"
    }
   },
   "outputs": [],
   "source": [
    "# Simple Cross-Validation: Split the data set into training and test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T00:57:15.612445Z",
     "start_time": "2023-08-02T00:57:15.607020Z"
    }
   },
   "outputs": [],
   "source": [
    "# GENERAL get layer dimensions for any model!\n",
    "def print_layers_dims(model):\n",
    "    l_layers = model.layers\n",
    "    # Note None is ALWAYS batch_size\n",
    "    for i in range(len(l_layers)):\n",
    "        print(l_layers[i])\n",
    "        print('Input Shape: ', l_layers[i].input_shape, 'Output Shape: ', l_layers[i].output_shape)\n",
    "\n",
    "# GENERAL save model to disk function!\n",
    "def save_model(fileModelJSON,fileWeights):\n",
    "    #print(\"Saving model to disk: \",fileModelJSON,\"and\",fileWeights)\n",
    "    #have h5py installed\n",
    "    if Path(fileModelJSON).is_file():\n",
    "        os.remove(fileModelJSON)\n",
    "    json_string = model.to_json()\n",
    "    with open(fileModelJSON,'w' ) as f:\n",
    "        json.dump(json_string, f)\n",
    "    if Path(fileWeights).is_file():\n",
    "        os.remove(fileWeights)\n",
    "    model.save_weights(fileWeights)\n",
    "    \n",
    "\n",
    "# GENERAL load model from disk function!\n",
    "def load_model(fileModelJSON,fileWeights):\n",
    "    #print(\"Saving model to disk: \",fileModelJSON,\"and\",fileWeights)\n",
    "    with open(fileModelJSON, 'r') as f:\n",
    "         model_json = json.load(f)\n",
    "         model = model_from_json(model_json)\n",
    "    \n",
    "    model.load_weights(fileWeights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture 1 - Simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T00:57:17.065089Z",
     "start_time": "2023-08-02T00:57:17.059774Z"
    }
   },
   "outputs": [],
   "source": [
    "## Deep Learning model Definition --- A --- (Simple LSTM)\n",
    "\n",
    "\n",
    "def simple_lstm(max_len=75, emb_dim=32, max_vocab_len=100, lstm_output_size=32, W_reg=regularizers.l2(1e-4)):\n",
    "    # Input\n",
    "    main_input = Input(shape=(max_len,), dtype='int32', name='main_input')\n",
    "    # Embedding layer\n",
    "    emb = Embedding(input_dim=max_vocab_len, output_dim=emb_dim, input_length=max_len)(main_input) \n",
    "    \n",
    "    # LSTM layer\n",
    "    lstm = LSTM(lstm_output_size)(emb)\n",
    "    lstm = Dropout(0.5)(lstm)\n",
    "    \n",
    "    # Output layer (last fully connected layer)\n",
    "    output = Dense(1, activation='sigmoid', name='output')(lstm)\n",
    "\n",
    "    # Compile model and define optimizer\n",
    "    model = Model(inputs=[main_input], outputs=[output])\n",
    "    adam = tf.keras.optimizers.legacy.Adam(learning_rate=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T01:00:52.322922Z",
     "start_time": "2023-08-02T00:57:19.226552Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit model and Cross-Validation, ARCHITECTURE 1 SIMPLE LSTM\n",
    "nb_epoch = 3\n",
    "batch_size = 32\n",
    "\n",
    "# Your code here.... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture 2 - 1D Convolution and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T20:45:37.211273Z",
     "start_time": "2023-08-01T20:45:37.205338Z"
    }
   },
   "outputs": [],
   "source": [
    "## Deep Learning model Definition --- B --- (1D Convolution and LSTM)\n",
    "\n",
    "def lstm_conv(max_len=75, emb_dim=32, max_vocab_len=100, lstm_output_size=32, W_reg=regularizers.l2(1e-4)):\n",
    "    # Input\n",
    "    main_input = Input(shape=(max_len,), dtype='int32', name='main_input')\n",
    "    # Embedding layer\n",
    "    emb = Embedding(input_dim=max_vocab_len, output_dim=emb_dim, input_length=max_len,\n",
    "                activity_regularizer=W_reg)(main_input) \n",
    "    emb = Dropout(0.25)(emb)\n",
    "\n",
    "    # Conv layer\n",
    "    conv = Convolution1D(filters=256, \\\n",
    "                     kernel_size=5)(emb)\n",
    "    conv = ELU()(conv)\n",
    "\n",
    "    conv = MaxPooling1D(pool_size=4)(conv)\n",
    "    #conv = BatchNormalization(mode=0)(conv)\n",
    "    conv = Dropout(0.5)(conv)\n",
    "\n",
    "    # LSTM layer\n",
    "    lstm = LSTM(lstm_output_size)(conv)\n",
    "    lstm = Dropout(0.5)(lstm)\n",
    "    \n",
    "    # Output layer (last fully connected layer)\n",
    "    output = Dense(1, activation='sigmoid', name='output')(lstm)\n",
    "\n",
    "    # Compile model and define optimizer\n",
    "    model = Model([main_input], [output])\n",
    "    adam = tf.keras.optimizers.legacy.Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T20:45:38.831738Z",
     "start_time": "2023-08-01T20:45:38.751936Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit model and Cross-Validation, ARCHITECTURE 2 CONV + LSTM\n",
    "nb_epoch = 5\n",
    "batch_size = 32\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture 3 - 1D Convolutions and Fully Connected Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deep Learning model Definition --- C --- (1D Convolutions and Fully Connected Layers)\n",
    "\n",
    "def conv_fully(max_len=75, emb_dim=32, max_vocab_len=100, W_reg=regularizers.l2(1e-4)):\n",
    "    # Input\n",
    "    main_input = Input(shape=(max_len,), dtype='int32', name='main_input')\n",
    "    # Embedding layer\n",
    "    emb = Embedding(input_dim=max_vocab_len, output_dim=emb_dim, input_length=max_len,\n",
    "                activity_regularizer=W_reg)(main_input) \n",
    "    emb = Dropout(0.25)(emb)\n",
    "\n",
    "    \n",
    "    def sum_1d(X):\n",
    "        return K.sum(X, axis=1)\n",
    "    \n",
    "    def get_conv_layer(emb, filter_length=5, nb_filter=256):\n",
    "        # Conv layer\n",
    "        conv = Convolution1D(kernel_size=filter_length, filters=nb_filter)(emb)\n",
    "        conv = ELU()(conv)\n",
    "\n",
    "        conv = Lambda(sum_1d, output_shape=(nb_filter,))(conv)\n",
    "        #conv = BatchNormalization(mode=0)(conv)\n",
    "        conv = Dropout(0.5)(conv)\n",
    "        return conv\n",
    "        \n",
    "    # Multiple Conv Layers\n",
    "    \n",
    "    # calling custom conv function from above\n",
    "    conv1 = get_conv_layer(emb, filter_length=2, nb_filter=256)\n",
    "    conv2 = get_conv_layer(emb, filter_length=3, nb_filter=256)\n",
    "    conv3 = get_conv_layer(emb, filter_length=4, nb_filter=256)\n",
    "    conv4 = get_conv_layer(emb, filter_length=5, nb_filter=256)\n",
    "\n",
    "    # Fully Connected Layers\n",
    "    merged = tf.keras.layers.Concatenate()([conv1,conv2,conv3,conv4])\n",
    "\n",
    "    hidden1 = Dense(1024)(merged)\n",
    "    hidden1 = ELU()(hidden1)\n",
    "    hidden1 = BatchNormalization()(hidden1)\n",
    "    hidden1 = Dropout(0.5)(hidden1)\n",
    "\n",
    "    hidden2 = Dense(1024)(hidden1)\n",
    "    hidden2 = ELU()(hidden2)\n",
    "    hidden2 = BatchNormalization()(hidden2)\n",
    "    hidden2 = Dropout(0.5)(hidden2)\n",
    "    \n",
    "    # Output layer (last fully connected layer)\n",
    "    output = Dense(1, activation='sigmoid', name='output')(hidden2)\n",
    "\n",
    "    # Compile model and define optimizer\n",
    "    model = Model([main_input], [output])\n",
    "    adam = tf.keras.optimizers.legacy.Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model and Cross-Validation, ARCHITECTURE 3 CONV + FULLY CONNECTED\n",
    "nb_epoch = 5\n",
    "batch_size = 32\n",
    "\n",
    "# Your code here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get probabilities of target predictions\n",
    "target_proba = model.predict(X_test, batch_size=1)\n",
    "target_proba[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a new prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_url_mal = \"naureen.net/etisalat.ae/index2.php\"\n",
    "test_url_benign = \"sixt.com/php/reservation?language=en_US\"\n",
    "\n",
    "url = test_url_benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert raw URL string in list of lists where characters that are contained in \"printable\" are stored encoded as integer \n",
    "url_int_tokens = [[printable.index(x) + 1 for x in url if x in printable]]\n",
    "\n",
    "# Step 2: Cut URL string at max_len or pad with zeros if shorter\n",
    "max_len=75\n",
    "X = sequence.pad_sequences(url_int_tokens, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_proba = model.predict(X, batch_size=5)\n",
    "def print_result(proba):\n",
    "    if proba > 0.5:\n",
    "        return \"malicious\"\n",
    "    else:\n",
    "        return \"benign\"\n",
    "print(\"Test URL:\", url, \"is\", print_result(target_proba[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
