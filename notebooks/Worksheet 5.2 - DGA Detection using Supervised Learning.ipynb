{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/GTK_Logo_Social Icon.jpg\" width=175 align=\"right\" />\n",
    "\n",
    "# Worksheet 5.2: DGA Detection\n",
    "This worksheet covers concepts covered in Module 5 on Supervised Learning.  It should take no more than 40-60 minutes to complete.  Please raise your hand if you get stuck.  \n",
    "\n",
    "## Import the Libraries\n",
    "For this exercise, we will be using:\n",
    "* Pandas (https://pandas.pydata.org/pandas-docs/stable/)\n",
    "* Numpy (https://docs.scipy.org/doc/numpy/reference/)\n",
    "* Matplotlib (https://matplotlib.org/)\n",
    "* Scikit-learn (https://scikit-learn.org/stable/documentation.html)\n",
    "* YellowBrick (https://www.scikit-yb.org/en/latest/)\n",
    "* Seaborn (https://seaborn.pydata.org)\n",
    "* Lime (https://github.com/marcotcr/lime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T14:51:08.092568Z",
     "start_time": "2023-08-01T14:51:08.087009Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Libraries - Make sure to run this cell!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn import feature_extraction, tree, model_selection, metrics\n",
    "#from yellowbrick.classifier import ClassificationReport\n",
    "#from yellowbrick.classifier import ConfusionMatrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "\n",
    "import lime\n",
    "import io\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worksheet - DGA Detection using Machine Learning\n",
    "\n",
    "This worksheet is a step-by-step guide on how to detect domains that were generated using \"Domain Generation Algorithm\" (DGA). We will walk you through the process of transforming raw domain strings to Machine Learning features and creating a decision tree classifer which you will use to determine whether a given domain is legit or not. Once you have implemented the classifier, the worksheet will walk you through evaluating your model.  \n",
    "\n",
    "Overview 2 main steps:\n",
    "\n",
    "1. **Feature Engineering** - from raw domain strings to numeric Machine Learning features using DataFrame manipulations\n",
    "2. **Machine Learning Classification** - predict whether a domain is legit or not using a Decision Tree Classifier\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "**DGA - Background**\n",
    "\n",
    "\"Various families of malware use domain generation\n",
    "algorithms (DGAs) to generate a large number of pseudo-random\n",
    "domain names to connect to a command and control (C2) server.\n",
    "In order to block DGA C2 traffic, security organizations must\n",
    "first discover the algorithm by reverse engineering malware\n",
    "samples, then generate a list of domains for a given seed. The\n",
    "domains are then either preregistered, sink-holed or published\n",
    "in a DNS blacklist. This process is not only tedious, but can\n",
    "be readily circumvented by malware authors. An alternative\n",
    "approach to stop malware from using DGAs is to intercept DNS\n",
    "queries on a network and predict whether domains are DGA\n",
    "generated. Much of the previous work in DGA detection is based\n",
    "on finding groupings of like domains and using their statistical\n",
    "properties to determine if they are DGA generated. However,\n",
    "these techniques are run over large time windows and cannot be\n",
    "used for real-time detection and prevention. In addition, many of\n",
    "these techniques also use contextual information such as passive\n",
    "DNS and aggregations of all NXDomains throughout a network.\n",
    "Such requirements are not only costly to integrate, they may not\n",
    "be possible due to real-world constraints of many systems (such\n",
    "as endpoint detection). An alternative to these systems is a much\n",
    "harder problem: detect DGA generation on a per domain basis\n",
    "with no information except for the domain name. Previous work\n",
    "to solve this harder problem exhibits poor performance and many\n",
    "of these systems rely heavily on manual creation of features;\n",
    "a time consuming process that can easily be circumvented by\n",
    "malware authors...\"    \n",
    "[Citation: Woodbridge et. al 2016: \"Predicting Domain Generation Algorithms with Long Short-Term Memory Networks\"]\n",
    "\n",
    "A better alternative for real-world deployment would be to use \"featureless deep learning\" - We have a separate notebook where you can see how this can be implemented!\n",
    "\n",
    "**However, let's learn the basics first!!!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Feature Engineered Data and Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Features and Labels\n",
    "\n",
    "If you got stuck in Part 1, please simply load the feature matrix we prepared for you, so you can move on to Part 2 and train a Decision Tree Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isDGA\n",
      "dga      1000\n",
      "legit    1000\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isDGA</th>\n",
       "      <th>length</th>\n",
       "      <th>digits</th>\n",
       "      <th>entropy</th>\n",
       "      <th>vowel-cons</th>\n",
       "      <th>firstDigitIndex</th>\n",
       "      <th>ngrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dga</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3.546594</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0</td>\n",
       "      <td>744.670940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dga</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "      <td>4.132944</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>715.217265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dga</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>1918.797619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dga</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>4.180833</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>1</td>\n",
       "      <td>682.269402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dga</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>3.834963</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2</td>\n",
       "      <td>544.178140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  isDGA  length  digits   entropy  vowel-cons  firstDigitIndex       ngrams\n",
       "0   dga      13       0  3.546594    0.083333                0   744.670940\n",
       "1   dga      26      10  4.132944    0.333333                1   715.217265\n",
       "2   dga       8       0  2.500000    0.333333                0  1918.797619\n",
       "3   dga      26       7  4.180833    0.357143                1   682.269402\n",
       "4   dga      24       9  3.834963    0.666667                2   544.178140"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load full dataset\n",
    "df_final = pd.read_csv('../data/dga_features_final_df.csv')\n",
    "\n",
    "#If you didn't get a working dataset, uncomment this line\n",
    "#df_final = pd.read_csv('../data/our_data_dga_features_final_df.csv')\n",
    "\n",
    "print(df_final['isDGA'].value_counts())\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning - Supervised Classification\n",
    "\n",
    "To learn simple classification procedures using [sklearn](http://scikit-learn.org/stable/) we have split the work flow into 5 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Prepare Feature matrix and ```target``` vector containing the URL labels\n",
    "\n",
    "- In statistics, the feature matrix is often referred to as ```X```\n",
    "- target is a vector containing the labels for each URL (often also called ```y``` in statistics)\n",
    "- In sklearn both the input and target can either be a pandas DataFrame/Series or numpy array/vector respectively (can't be lists!)\n",
    "\n",
    "Tasks:\n",
    "- 1.1 assign 'isDGA' column to a pandas Series named 'target'\n",
    "- 1.2 encode the strings as digits using sklearn ```LabelEncoder``` \n",
    "- 1.3 drop 'isDGA' column from ```df_final``` DataFrame and name the resulting pandas DataFrame ```feature_matrix```\n",
    "\n",
    "## 1.1 Create a variable named 'target' \n",
    "that contains whether the row was dga or legit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = # Your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Encode strings as digits in the target variable\n",
    "\n",
    "LabelEncoder is an encoder from sklearn that will transform a target vector (not the features) into integers.\n",
    "\n",
    "[label encoder doc](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder_model = LabelEncoder()\n",
    "encoded_target = label_encoder_model.fit_transform(#your code here)\n",
    "\n",
    "# print our classes with the .classes_ method\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at encoded targets\n",
    "encoded_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason we like to encode targets using sklearn, is that once we are finished modeling, we can use the same model to turn it back into strings. This is really useful when plotting metrics or looking at the results (especially when you have more than one class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform our encoded targets back into strings (we will use this when we are evaluating the model later\n",
    "label_encoder_model.inverse_transform(encoded_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Create the Feature Matrix\n",
    "\n",
    "In order to train a model you have to separate the features from the target in to separate objects. Drop **isDGA** column from ```df_final``` DataFrame and name the resulting pandas DataFrame ```feature_matrix```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creata a list of our feature names for plotting later and if we need to pull the features again from the full dataframe.\n",
    "feature_names = feature_matrix.columns.to_list()\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Test-Train split\n",
    "\n",
    "Tasks:\n",
    "- split your `feature_matrix` and `target` vector into **train** and **test** subsets using sklearn [model_selection.train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "Output of the split should be 2 complete sets of data separated by features and labels: \n",
    " - feature_matrix_train: 75% of the feature matrix (data)\n",
    " - feature_matrix_test: the remaining 25% of the feature matrix\n",
    " - target_train: the labels for the train features\n",
    " - target_test: the labels for the test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data set into training and test data\n",
    "feature_matrix_train, feature_matrix_test, target_train, target_test = (model_selection.train_test_split(#YOUR CODE HERE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Train the model and make a prediction\n",
    "\n",
    "Finally, we have prepared and segmented the data. Let's start classifying!!   \n",
    "\n",
    "Tasks:\n",
    "-  Use the sklearn [tree.DecisionTreeClassfier()](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), create a decision tree with default parameters, and train it using the ```.fit()``` function with ```feature_matrix_train``` and ```target_train``` data.\n",
    "-  Next, pull a few random rows from the data and see if your classifier got it correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the decision tree based on the entropy criterion\n",
    "d_tree_model = #YOURCODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! You trained the model. Now extract a row from the test set to see if the model can predict the correct answer by comparing it to the test target (ground truth). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a row from the test data\n",
    "\n",
    "row_number = 14\n",
    "test_feature = feature_matrix_test[row_number:row_number+1]\n",
    "\n",
    "# Make the prediction\n",
    "pred = d_tree_model.predict(test_feature)\n",
    "\n",
    "#Let's go back to having strings in the targets to compare \n",
    "#b/c it'e easier to read. We can do this using the inverse_transform from the label encoder model\n",
    "pred_string = label_encoder_model.inverse_transform(pred)\n",
    "\n",
    "# pull out the ground truth for this row\n",
    "test_target = target_test[row_number:row_number+1]\n",
    "# transfrom to a string\n",
    "test_target_string = label_encoder_model.inverse_transform(test_target)\n",
    "\n",
    "                                                    \n",
    "# print the results and the ground truth\n",
    "print('Predicted class:', pred_string)\n",
    "print('Ground truth class:', test_target_string)\n",
    "print('Accurate prediction?', pred_string == test_target_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Assess model accuracy with multiple reports of the confusion matrix and accuracy\n",
    "\n",
    "Tasks:\n",
    "- Make predictions for all your data. Call the ```.predict()``` method on the clf with your training data ```featre_matrix_train``` and store the results in a variable called ```target_pred```.\n",
    "- Use sklearn [metrics.accuracy_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) to determine your models accuracy. Detailed Instruction:\n",
    "    - Use your trained model to predict the labels of your test data ```feature_matrix_test```. Run ```.predict()``` method on the clf with your test data ```feature_matrix_test``` and store the results in a variable called ```target_pred```.. \n",
    "    - Then calculate the accuracy using ```target_test``` (which are the true labels/groundtruth) AND your models predictions on the test portion ```target_pred``` as inputs. The advantage here is to see how your model performs on new data it has not been seen during the training phase. The fair approach here is a simple **cross-validation**!\n",
    "    \n",
    "- Print out the confusion matrix using [metrics.confusion_matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
    "- Use Yellowbrick to visualize the classification report and confusion matrix. (https://www.scikit-yb.org/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fair approach: make prediction on test data portion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report...neat summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Visualizing your Tree\n",
    "As an optional step, you can actually visualize your tree.  The following code will generate a graph of your decision tree.  You will need graphviz (http://www.graphviz.org) and pydotplus (or pydot) installed for this to work.\n",
    "The Griffon VM has this installed already, but if you try this on a Mac, or Linux machine you will need to install graphviz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These libraries are used to visualize the decision tree and require that you have GraphViz\n",
    "# and pydot or pydotplus installed on your computer.\n",
    "\n",
    "from IPython.core.display import Image\n",
    "import pydotplus as pydot\n",
    "\n",
    "\n",
    "dot_data = io.StringIO() \n",
    "tree.export_graphviz(dga_detect_tree_model, out_file=dot_data, \n",
    "                     feature_names=['length', 'digits', 'entropy', 'vowel-cons', 'firstDigitIndex','ngrams'],\n",
    "                    filled=True, rounded=True,  \n",
    "                    special_characters=True) \n",
    "\n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue()) \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Models\n",
    "Now that you've built a Decision Tree, let's try out two other classifiers and see how they perform on this data.  For this next exercise, create classifiers using:\n",
    "\n",
    "* Support Vector Machine\n",
    "* Random Forest\n",
    "* K-Nearest Neighbors (http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)  \n",
    "\n",
    "Once you've done that, run the various performance metrics to determine which classifier works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the Random Forest Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, create the SVM classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally the knn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain a Prediction\n",
    "In the example below, you can use LIME to explain how a classifier arrived at its prediction.  Try running LIME with the various classifiers you've created and various rows to see how it functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime.lime_tabular\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(feature_matrix_train, \n",
    "                                                   feature_names=['length', 'digits', 'entropy', 'vowel-cons', 'firstDigitIndex','ngrams'], \n",
    "                                                   class_names=['legit', 'isDGA'], \n",
    "                                                   discretize_continuous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function assumes your model is names random_forest_clf. change that variable to what you named your model\n",
    "exp = explainer.explain_instance(feature_matrix_test.iloc[5], \n",
    "                                 random_forest_clf.predict_proba, \n",
    "                                 num_features=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(show_table=True, show_all=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
